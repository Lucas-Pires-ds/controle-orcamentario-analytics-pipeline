# ğŸ“Š Projeto de Controle OrÃ§amentÃ¡rio â€” Pipeline ETL e Analytics

## ğŸ“Œ TL;DR
- Pipeline ETL completo em **SQL Server** (Bronze â†’ Silver â†’ Gold)
- Forte foco em **qualidade de dados**, integridade referencial e rastreabilidade
- Modelo dimensional para anÃ¡lise financeira e orÃ§amentÃ¡ria
- Camada Gold composta por **3 views analÃ­ticas**:
  - **OrÃ§amento**
  - **LanÃ§amentos**
  - **Realizado**
- Cruzamento **OrÃ§ado vs Realizado realizado no Power BI**
- MÃ©tricas prontas para consumo no **Power BI**, com mÃ­nima lÃ³gica em DAX

---

## ğŸ§­ VisÃ£o Geral

Este projeto simula um **pipeline de dados financeiro-orÃ§amentÃ¡rio**, cobrindo desde a ingestÃ£o de dados brutos atÃ© a entrega de um **modelo analÃ­tico confiÃ¡vel para consumo no Power BI**.

O objetivo nÃ£o Ã© apenas gerar dashboards, mas estruturar dados de forma consistente, tratando problemas reais como:
- Baixa padronizaÃ§Ã£o
- Falhas de integridade
- InconsistÃªncias semÃ¢nticas
- AusÃªncia de controle de qualidade antes da anÃ¡lise

O pipeline foi desenvolvido utilizando **SQL Server**, **Python** e **Power BI**, com foco em decisÃµes tÃ©cnicas explÃ­citas e defensivas, prÃ³ximas do que ocorre em ambientes corporativos.

---

## ğŸ¢ Contexto do NegÃ³cio â€” Sage

A **Sage** Ã© uma empresa fictÃ­cia do setor de serviÃ§os, criada como contexto para a construÃ§Ã£o e validaÃ§Ã£o do pipeline de dados apresentado neste projeto.

A empresa opera com mÃºltiplos **centros de custo** (administrativo, operaÃ§Ãµes e marketing), realiza **planejamento orÃ§amentÃ¡rio mensal** e registra **lanÃ§amentos financeiros diÃ¡rios** relacionados a fornecedores, campanhas e despesas operacionais.

Como ocorre em muitos ambientes corporativos, a base financeira apresenta problemas recorrentes na origem dos dados, tais como:
- Baixa padronizaÃ§Ã£o de campos na origem
- VariaÃ§Ãµes de texto e status sem padronizaÃ§Ã£o
- ReferÃªncias invÃ¡lidas a dimensÃµes analÃ­ticas
- AusÃªncia de validaÃ§Ãµes antes do consumo analÃ­tico

O projeto foi desenvolvido para estruturar, tratar e padronizar esses dados ao longo das camadas de ETL, viabilizando uma anÃ¡lise confiÃ¡vel de **OrÃ§ado vs Realizado**, tanto em nÃ­vel **mensal (visÃ£o executiva)** quanto **diÃ¡rio (acompanhamento intramÃªs)**, com regras de negÃ³cio e qualidade aplicadas ainda na camada de dados.

---

## ğŸ¯ Problema de NegÃ³cio

Empresas de serviÃ§os, como a Sage, frequentemente enfrentam desafios como:

- Dados financeiros vindos de mÃºltiplas fontes
- Falta de validaÃ§Ãµes antes da anÃ¡lise
- Dificuldade em garantir consistÃªncia entre categorias, centros de custo e campanhas
- Baixa confiabilidade nos indicadores financeiros e orÃ§amentÃ¡rios

Este projeto resolve esses pontos ao:

- Centralizar os dados em um pipeline Ãºnico
- Aplicar regras de saneamento ainda na camada de dados
- Garantir integridade referencial e semÃ¢ntica
- Entregar bases analÃ­ticas confiÃ¡veis para consumo no Power BI

---

## ğŸ—ï¸ Arquitetura de Dados

![Arquitetura do Pipeline de Dados](docs_e_imagens/diagrama_pipeline_de_dados.png)

O projeto segue o padrÃ£o **Medallion Architecture**, com responsabilidades bem definidas por camada.

---

## ğŸ¥‰ Camada Bronze (stg_)

ResponsÃ¡vel pela ingestÃ£o dos dados brutos.

- IngestÃ£o via **Python (Pandas) + BULK INSERT**
- Todas as colunas armazenadas como `VARCHAR(MAX)` ou `VARCHAR(200)`
- Nenhuma tipagem ou regra de negÃ³cio aplicada

**Objetivo:** garantir que a carga nunca falhe por incompatibilidade de tipos e preservar o dado original.

> Os caminhos utilizados nos comandos `BULK INSERT` sÃ£o parametrizÃ¡veis e devem ser ajustados conforme o ambiente local.

---

## ğŸ” TransformaÃ§Ãµes via Views (vw_)

As transformaÃ§Ãµes entre Bronze e Silver sÃ£o feitas por meio de **Views** no SQL Server.

BenefÃ­cios:
- Ajuste de regras sem reprocessar dados fÃ­sicos
- Auditoria e rastreabilidade das transformaÃ§Ãµes
- SeparaÃ§Ã£o clara entre ingestÃ£o e tratamento

---

## ğŸ¥ˆ Camada Silver (dim_ e fact_)

Camada responsÃ¡vel pela persistÃªncia dos dados tratados.

CaracterÃ­sticas:
- Dados tipados
- AplicaÃ§Ã£o de `PRIMARY KEY` e `FOREIGN KEY`
- Modelo dimensional em **Star Schema**

Essa camada representa a base confiÃ¡vel para consumo analÃ­tico.

---

## âœ… Framework de Qualidade de Dados

Antes da carga definitiva na Silver, foi realizado **Data Profiling** por meio de queries de diagnÃ³stico.

### Principais validaÃ§Ãµes aplicadas

- **Auditoria de EspaÃ§os**
  - `LEN(col) > LEN(TRIM(col))`
- **Sanidade de IDs**
  - IdentificaÃ§Ã£o de valores como `"101.0"` importados como string
- **ValidaÃ§Ã£o de DomÃ­nio**
  - Meses fora do intervalo vÃ¡lido (1â€“12)
- **Unicidade**
  - DetecÃ§Ã£o de chaves duplicadas (`GROUP BY + HAVING COUNT(*) > 1`)

Essas validaÃ§Ãµes evitam erros silenciosos e garantem confiabilidade antes da persistÃªncia fÃ­sica.

---

## ğŸ“ˆ Resultados do Processo de ETL

O processo de ETL nÃ£o teve como objetivo apenas mover dados entre camadas, mas **sanear, padronizar e tornar a base analÃ­tica confiÃ¡vel** antes do consumo no Power BI.

As intervenÃ§Ãµes realizadas ao longo das camadas Bronze, Silver e Gold foram guiadas por problemas concretos identificados no Data Profiling, com foco em reduzir risco analÃ­tico e garantir consistÃªncia dos indicadores.

---

### ğŸ§ª Principais Tratamentos Aplicados no ETL

| Tipo de validaÃ§Ã£o / tratamento      | EvidÃªncia identificada na Bronze            | AÃ§Ã£o aplicada no ETL                           | Impacto analÃ­tico |
|------------------------------------|---------------------------------------------|------------------------------------------------|-------------------|
| Datas nulas                        | Registros sem referÃªncia temporal            | Descarte controlado ainda na View              | Evita distorÃ§Ãµes em anÃ¡lises temporais |
| Centros de custo invÃ¡lidos         | IDs inexistentes nas dimensÃµes               | Uso de membro coringa `-1 (NÃƒO IDENTIFICADO)`  | Preserva valores financeiros sem violar FKs |
| IDs com resÃ­duos decimais          | Strings no formato `"101.0"`                 | ConversÃ£o `FLOAT â†’ INT`                        | Garante integridade das chaves |
| Status de pagamento inconsistentes | VariaÃ§Ãµes de case, gÃªnero e idioma           | NormalizaÃ§Ã£o semÃ¢ntica via `CASE WHEN`         | Indicadores consistentes no dashboard |
| Valores com sinal inconsistente    | Valores negativos sem estorno associado      | Tratamento com `ABS()` e redundÃ¢ncia defensiva | Evita interpretaÃ§Ã£o financeira incorreta |
| EspaÃ§os e ruÃ­dos textuais          | Strings com espaÃ§os extras                   | AplicaÃ§Ã£o de `TRIM()` e padronizaÃ§Ã£o de texto  | Melhora agrupamentos e filtros |

---

### ğŸ“Š Resultado Final do Pipeline

ApÃ³s a aplicaÃ§Ã£o das regras de ETL e qualidade de dados:

- 100% dos registros persistidos na camada Silver respeitam regras de tipagem e integridade referencial
- O modelo dimensional pode ser consumido diretamente no Power BI, sem necessidade de tratamentos adicionais em DAX
- As mÃ©tricas de **OrÃ§ado vs Realizado** refletem regras de negÃ³cio explÃ­citas e defensivas
- O risco de erros silenciosos em anÃ¡lises financeiras foi mitigado ainda na camada de dados

O valor do pipeline nÃ£o estÃ¡ apenas na visualizaÃ§Ã£o final, mas na **confiabilidade da base analÃ­tica construÃ­da**, garantindo que as anÃ¡lises reflitam o negÃ³cio de forma consistente e rastreÃ¡vel.

---

### CorreÃ§Ã£o de Tipagem na IngestÃ£o

Durante a ingestÃ£o, alguns identificadores numÃ©ricos foram importados como strings decimais (ex: `"101.0"`), o que impede a conversÃ£o direta para `INT` no SQL Server.

Para tratar esse cenÃ¡rio, foi aplicada a conversÃ£o:

CAST(CAST(col AS FLOAT) AS INT)

Essa abordagem garante a correta tipagem dos identificadores e evita falhas de conversÃ£o durante o processo de ETL.

---

### Tratamento e PadronizaÃ§Ã£o de Texto

Foi implementada uma lÃ³gica personalizada de padronizaÃ§Ã£o textual:

- Primeira letra maiÃºscula
- Demais letras minÃºsculas
- PreservaÃ§Ã£o de siglas (`RH`, `TI`)
- Tratamento correto de delimitadores (`Limpeza/ConservaÃ§Ã£o`)

O objetivo Ã© melhorar a leitura analÃ­tica sem alterar o significado dos dados.

---

### Integridade e Limpeza

- Registros com IDs nulos foram identificados como causa raiz de duplicidades
- Esses registros foram descartados ainda nas Views
- ValidaÃ§Ãµes garantem que toda categoria possua Centro de Custo vÃ¡lido antes da carga

---

## ğŸ§© Modelo Dimensional (Silver)

O modelo foi construÃ­do seguindo o padrÃ£o **Star Schema**, priorizando clareza e performance.

### DimensÃµes implementadas

- `dim_centro_custo`
- `dim_categoria` (FK para centro de custo)
- `dim_camp_marketing`
- `dim_fornecedores`

---

## ğŸ“„ Tabela Fato â€” fact_lancamentos

A tabela `fact_lancamentos` representa os lanÃ§amentos financeiros realizados.

### DiagnÃ³stico de Qualidade (PrÃ©-Carga)

Durante o profiling da `stg_lancamentos`, foram identificados:

- **Integridade Temporal**
  - 27 registros sem data (~0,6%)
- **Integridade Referencial**
  - 65 registros (~1,3%) sem centro de custo vÃ¡lido
- **Anomalias de Sinal**
  - Valores negativos sem correspondÃªncia com estorno
- **InconsistÃªncia SemÃ¢ntica**
  - Status duplicados por variaÃ§Ã£o de case e gÃªnero

---

### DecisÃµes de Engenharia

- **Descarte EstratÃ©gico**
  - Registros sem data removidos (baixo impacto financeiro)
- **Membro Coringa**
  - CriaÃ§Ã£o do registro `-1 (NÃƒO IDENTIFICADO)` em `dim_centro_custo`
- **RedundÃ¢ncia Defensiva**
  - `valor`: tratado com `ABS()` e `CHECK (> 0)`
  - `valor_original`: preservado para auditoria
- **NormalizaÃ§Ã£o de Status**
  - PadronizaÃ§Ã£o para apenas `Pago` e `Aberto`

---

### Status Final da fact_lancamentos

- Chave primÃ¡ria definida
- Integridade referencial garantida
- 100% dos registros vÃ¡lidos segundo regras de negÃ³cio

---

## ğŸ¥‡ Camada Gold â€” DecisÃµes AnalÃ­ticas

A camada Gold foi desenhada a partir das necessidades analÃ­ticas da Sage, com foco em **simplicidade, clareza semÃ¢ntica e reduÃ§Ã£o de lÃ³gica no Power BI**.

Diferente de uma camada puramente agregada, a Gold foi estruturada em **trÃªs views analÃ­ticas independentes**, cada uma com responsabilidade bem definida.  
O **cruzamento entre orÃ§amento e realizado Ã© realizado no Power BI**, e nÃ£o na camada de dados, por decisÃ£o arquitetural consciente.

---

### ğŸ“Š vw_gold_orcamento

Responsabilidades:

- ConsolidaÃ§Ã£o mensal de orÃ§amento

- CÃ¡lculo de **YTD**

- Pesos relativos por **centro de custo** e **categoria**

- MÃ©dia histÃ³rica mensal

- Flag de valores atÃ­picos via desvio em relaÃ§Ã£o Ã  mÃ©dia

- ProteÃ§Ã£o contra divisÃ£o por zero (NULLIF)

- Nenhum cruzamento com realizado

### ğŸ“„ vw_gold_lancamentos

Responsabilidades:

- VisÃ£o detalhada e auditÃ¡vel dos lanÃ§amentos diÃ¡rios

- PreservaÃ§Ã£o de valor_original e valor tratado

- Flags de centro de custo coringa

- Enriquecimento dimensional completo (centro de custo, categoria, fornecedor, campanha)

- Nenhuma agregaÃ§Ã£o (base para drill-down)

### ğŸ“ˆ vw_gold_realizado

Responsabilidades:

- ConsolidaÃ§Ã£o mensal do realizado

- Uso consciente da dim_calendario para continuidade temporal

- MÃ©tricas avanÃ§adas:

  - YTD

  - MoM absoluto e percentual

  - YoY absoluto e percentual

  - MÃ©dia mensal

  - Pesos relativos

  - Flags de anomalia

- ManutenÃ§Ã£o da rastreabilidade do centro de custo coringa

- Nenhum cÃ¡lculo de OrÃ§ado vs Realizado

### Regras AnalÃ­ticas 

- Uso de `COALESCE` para consistÃªncia visual

- PrevenÃ§Ã£o de divisÃ£o por zero com `NULLIF`

- Continuidade temporal garantida via `dim_calendario`

- Flags explÃ­citas para valores atÃ­picos

- CÃ¡lculos complexos concentrados na Gold quando necessÃ¡rio, o restante serÃ¡ feito no Power BI

---

## ğŸ› ï¸ Stack Utilizada

- **SQL Server** â€” ETL e modelagem dimensional
- **Python (Pandas)** â€” ingestÃ£o e dados sintÃ©ticos
- **Power BI** â€” visualizaÃ§Ã£o
- **Git / GitHub** â€” versionamento e documentaÃ§Ã£o

---

## ğŸ“Œ Objetivo do Projeto

Este projeto foi desenvolvido para consolidar estudos em **AnÃ¡lise de Dados, BI e Engenharia AnalÃ­tica**, aplicando conceitos em um cenÃ¡rio financeiro realista.

O foco estÃ¡ no processo:
- DecisÃµes tÃ©cnicas explÃ­citas
- Tratamento de dados imperfeitos
- ConstruÃ§Ã£o de uma base analÃ­tica confiÃ¡vel

---

## ğŸ“ PrÃ³ximos Passos

- Evoluir anÃ¡lises no Power BI
- Publicar dashboards finais

> **Status:** projeto em desenvolvimento contÃ­nuo.

ğŸ“¬ Fique Ã  vontade para explorar o repositÃ³rio e enviar feedbacks ou sugestÃµes.
